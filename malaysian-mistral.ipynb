{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a55c3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "from huggingface_hub import InferenceClient\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6f1d09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15715it [00:00, 319111.74it/s]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "with open('nsfw-tweets-en-ms.jsonl') as f:\n",
    "    \n",
    "    for x in tqdm(f):\n",
    "        data.append(json.loads(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b664ef8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3c86f2ab35641c3956629543628758b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2')\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'mistralai/Mistral-7B-Instruct-v0.2', \n",
    "    use_flash_attention_2 = True, \n",
    "    torch_dtype = torch.float16,\n",
    "    device_map=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e45c7e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(row):\n",
    "    system_prompt = f\"\"\"\n",
    "\n",
    "    text: {row['en']}\n",
    "\n",
    "    If the text shows any sign of information, label it as 'informative'.\n",
    "    If the text shows any sign of safe for work, label it as 'safe for work'.\n",
    "    If the text shows any sign of lgbt, label it as 'lgbt'.\n",
    "    If the text shows any sign of sexist, label it as 'sexist'.\n",
    "    If the text shows any sign of violence, label it as 'violence'.\n",
    "    If the text shows any sign of racist, label it as 'racist'.\n",
    "    If the text shows any sign of porn, label it as 'porn'.\n",
    "    If the text shows any sign of psychiatric or mental illness, label it as 'psychiatric or mental illness'.\n",
    "    If the text shows any sign of self-harm, label it as 'self-harm'.\n",
    "    If the text shows any sign of hate, label it as 'hate'.\n",
    "    If the text shows any sign of religion insult, label it as 'religion insult'.\n",
    "    If the text shows any sign of harassment, label it as 'harassment'.\n",
    "\n",
    "    Only use the label from above choice.\n",
    "\n",
    "    return the result in JSON format {'label', 'explain'}\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": f\"{system_prompt}\"}]\n",
    "\n",
    "    encodeds = tokenizer.apply_chat_template(messages,tokenize = False)\n",
    "\n",
    "\n",
    "    return encodeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30fc3270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(row):\n",
    "    system_prompt = f\"\"\"\n",
    "\n",
    "    text: {row['Content']}\n",
    "\n",
    "    If the text shows any sign of prejudice, stereotyping, or discrimination on the basis of sex:, label it as 'sexist'.\n",
    "    If the text shows any sign of content that threatens, incites, glorifies, or expresses desire for violence or harm., label it as 'violence'.\n",
    "    If the text shows any sign of prejudice, discrimination, or antagonism by an individual, community, or institution against a person or people on the basis of their membership of a particular racial or ethnic group, label it as 'racist'.\n",
    "    If the text shows any sign of content depicting adult nudity or sexual behavior that is pornographic or intended to cause sexual arousal, label it as 'porn'.\n",
    "    If the text shows any sign of psychiatric or mental illness, label it as 'psychiatric or mental illness'.\n",
    "    If the text shows any sign of promotion, or otherwise encourage, suicide or self-harm, label it as 'self-harm'.\n",
    "    If the text shows any sign of harassment, label it as 'harassment'.\n",
    "    If the text does not show any sign of violation and safe for work, label it as 'safe for work'\n",
    "\n",
    "    Only use the label from above choice.\n",
    "\n",
    "    return the result in JSON format {'label', 'explain'}\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": f\"{system_prompt}\"}]\n",
    "\n",
    "    encodeds = tokenizer.apply_chat_template(messages,tokenize = False)\n",
    "\n",
    "\n",
    "    return encodeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cad7d613",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_kwargs = dict(\n",
    "    temperature=1,\n",
    "    max_new_tokens=4096,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.0,\n",
    "    do_sample=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "399695b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c6f885da0a94a55bedd8d22cd3433ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "470852e905ad4434b706bd3e45ffb5dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.79M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f307feaafb041159dd20fb901254e70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bf1e5b936d34871bd36e294fe3c6668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/637 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e11ca553f67f45df9dfc40ef623e2050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b9a7ef6ded44f6e81ccfbe04713a24c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "587248c7038845e79361bef93e964aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e08ee53d3caf449cb48104a155d25a6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c4afff12bd4a4e828730b21ac73041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd8f8e49beb64ae2a6f660e12b336c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32056252d0ab447db264a77f920dca87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mesolitica/mallam-small-32768-fpf-v3\",)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mesolitica/mallam-small-32768-fpf-v3\",\n",
    "                                               use_flash_attention_2 = True, \n",
    "                                                torch_dtype = torch.float16).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90649a75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–‰         | 301/3143 [23:11<3:39:02,  4.62s/it]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = 5\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for i in tqdm(range(0, len(data), batch_size)):\n",
    "        batch = data[i:i+batch_size]\n",
    "        encodeds = [format_prompt(row) for row in batch]\n",
    "        \n",
    "        model_inputs = tokenizer(encodeds,padding=True,return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "        generated_ids = model.generate(**model_inputs,temperature=1,max_new_tokens=1000,top_p=0.95,repetition_penalty=1.0,do_sample=True,pad_token_id = tokenizer.eos_token_id)\n",
    "        decoded = tokenizer.batch_decode(generated_ids,skip_special_tokens=True,)\n",
    "        \n",
    "                with open('nsfw-inference-mistral.jsonl', 'a') as f:\n",
    "            for i in range(len(decoded)):\n",
    "                batch[i]['output'] = decoded[i].split('[/INST]')[1].strip()\n",
    "                json.dump(batch[i], f)\n",
    "                f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
